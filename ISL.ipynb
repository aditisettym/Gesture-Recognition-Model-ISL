{"cells":[{"cell_type":"markdown","metadata":{"id":"RPNbDPVnlxxw"},"source":["# Dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lv6-qoFelxx2","outputId":"303182c2-f849-4f1f-fa22-53a20e96ac83"},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/kshithijkeshav/Programs/Git_Folder/Mini-project-Sem5/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n","  warnings.warn(\n"]}],"source":["import tensorflow as tf\n","import cv2\n","import numpy as np\n","import mediapipe as mp\n","import os\n","import time\n","import sklearn\n"]},{"cell_type":"markdown","metadata":{"id":"BFp549mwlxx6"},"source":["# Mediapipe And OpenCV"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ROzlEKvslxx7"},"outputs":[],"source":["#mp_hands=mp.solutions.objectron\n","mp_holistic=mp.solutions.holistic\n","mp_drawing=mp.solutions.drawing_utils"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"syAwG0DJlxx9"},"outputs":[],"source":["def mediapipe_detector(image,model):\n","    image=cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n","    image.flags.writeable=False\n","    results=model.process(image)\n","    image.flags.writeable=True\n","    image=cv2.cvtColor(image,cv2.COLOR_RGB2BGR)\n","    return image,results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rj3O0MHBlxx-"},"outputs":[],"source":["def draw_landmarks(image,results):\n","    #mp_drawing.draw_landmarks(image,results.face_landmarks,mp_holistic.FACEMESH_CONTOURS)\n","    #mp_drawing.draw_landmarks(image,results.pose_landmarks,mp_holistic.POSE_CONNECTIONS)\n","    mp_drawing.draw_landmarks(image,results.left_hand_landmarks,mp_holistic.HAND_CONNECTIONS)\n","    mp_drawing.draw_landmarks(image,results.right_hand_landmarks,mp_holistic.HAND_CONNECTIONS)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["parameters"],"id":"CkD3U2tMlxyA"},"outputs":[],"source":["cam=cv2.VideoCapture(0)\n","with mp_holistic.Holistic(min_detection_confidence=0.5,min_tracking_confidence=0.5) as holistic:\n","    while cam.isOpened():\n","        ret,frame=cam.read()\n","        image,results=mediapipe_detector(frame,holistic)\n","        #print(results)\n","        draw_landmarks(image,results)\n","        cv2.imshow('Python Camera',image)\n","        if cv2.waitKey(10) & 0xFF==ord('q'):\n","            break\n","    cam.release()\n","    cv2.destroyWindow('Python Camera')\n","    cv2.waitKey(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Utz8YVFDlxyC"},"outputs":[],"source":["len(results.pose_landmarks.landmark)"]},{"cell_type":"markdown","metadata":{"id":"nmS2U3YMlxyG"},"source":["# Data to NP ARRAY"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j1oMy0fhlxyG"},"outputs":[],"source":["#pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(132)\n","#face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(1404)\n","lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n","rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jcC6wZj4lxyH"},"outputs":[],"source":["def extract_keypoints(results):\n","    #face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(1404)\n","    #pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n","    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n","    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n","    return np.concatenate([lh, rh])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6bbuBikKlxyI"},"outputs":[],"source":["extract_keypoints(results)"]},{"cell_type":"markdown","metadata":{"id":"Gn9vTAUklxyI"},"source":["# Save Data to file /Making data for the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HBM7U9BylxyJ"},"outputs":[],"source":["DATA_PATH = os.path.join('Final_Data')\n","no_of_sequences=40\n","sequence_length=45\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ZQtgM4WlxyJ"},"outputs":[],"source":["actions=np.array(['Hi','I Love You','How Are You','Good Morning','What Is Your Name','Sorry','Thank You','India'])\n","for action in actions:\n","    for sequence in range(no_of_sequences):\n","        try:\n","            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n","        except:\n","            pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j1sKSRhzlxyJ"},"outputs":[],"source":["cam = cv2.VideoCapture(0)\n","\n","with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n","    for action in actions:\n","        for sequence in range(no_of_sequences):\n","            for frame_no in range(sequence_length):\n","                ret, frame = cam.read()\n","                image, results = mediapipe_detector(frame, holistic)\n","                draw_landmarks(image, results)\n","\n","                # Convert 0-based index to 1-based for display\n","                display_sequence = sequence + 1\n","                display_frame = frame_no + 1\n","\n","                # Display start of collection message\n","                if frame_no == 0:\n","                    cv2.putText(image, '--- STARTING COLLECTION ---', (50, 100),\n","                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 4, cv2.LINE_AA)\n","                    cv2.putText(image, f'Collecting frames for \"{action}\" | Video #{display_sequence}', (50, 150),\n","                                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2, cv2.LINE_AA)\n","                    cv2.imshow('Python Camera', image)\n","                    cv2.waitKey(2000)  # Pause for 2 seconds\n","                else:\n","                    cv2.putText(image, f'Collecting \"{action}\" | Video #{display_sequence} | Frame {display_frame}', (50, 100),\n","                                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2, cv2.LINE_AA)\n","                    cv2.imshow('Python Camera', image)\n","\n","                # Save keypoints\n","                keypoints = extract_keypoints(results)\n","                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_no))\n","                np.save(npy_path, keypoints)\n","\n","                # Break on 'q'\n","                if cv2.waitKey(10) & 0xFF == ord('q'):\n","                    break\n","\n","cam.release()\n","cv2.destroyAllWindows()\n","cv2.waitKey(1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nhj3qSyklxyK"},"outputs":[],"source":["cam.release()\n","cv2.destroyAllWindows('Python Camera')\n","cv2.waitKey(1)"]},{"cell_type":"markdown","metadata":{"id":"Nd4YRr3ylxyK"},"source":["# Labeling the data using SKlearn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0wgP48GklxyK"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from tensorflow.keras.utils import to_categorical"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c6e5AhlmlxyK","outputId":"4d4e2d80-d5d5-4116-c73d-d91be2ab76b9"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'Hi': 0, 'I Love You': 1, 'How Are You': 2, 'Good Morning': 3, 'What Is Your Name': 4, 'Sorry': 5, 'Thank You': 6, 'India': 7}\n"]}],"source":["label_map={label: num for num,label in enumerate(actions)}\n","print(label_map)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"35UZGOkalxyL"},"outputs":[],"source":["sequences, labels = [], []\n","for action in actions:\n","    for sequence in range(no_of_sequences):\n","        window = []\n","        for frame_num in range(sequence_length):\n","            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n","            window.append(res)\n","        sequences.append(window)\n","        labels.append(label_map[action])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U9YfGyb_lxyL"},"outputs":[],"source":["X=np.array(sequences)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sBr5iBFSlxyL","outputId":"6f451b28-0111-41f4-c57b-32cae384d7a6"},"outputs":[{"data":{"text/plain":["(320, 45, 126)"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["X.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bb8KPuoalxyM"},"outputs":[],"source":["y = to_categorical(labels).astype(int)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I4qj57rUlxyM"},"outputs":[],"source":["y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Va2MbWWYlxyM"},"outputs":[],"source":["X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.05)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Br2mOCRXlxyM","outputId":"9889f248-7e82-4d5d-86de-d79f7e9b1e6a"},"outputs":[{"data":{"text/plain":["numpy.ndarray"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["type(X_train)"]},{"cell_type":"markdown","metadata":{"id":"HBlxIhtalxyM"},"source":["# Making the Actual model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uliVMTo5lxyN","outputId":"89d53c79-ccdb-4bd2-9ab4-40e9f81fa7f3"},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:From /var/folders/_v/d72hzd495g3bkw1wdcl6vrlh0000gn/T/ipykernel_1993/1473334969.py:2: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.config.list_physical_devices('GPU')` instead.\n","True\n"]}],"source":["import tensorflow as tf\n","print(tf.test.is_gpu_available())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mswn1g0OlxyN"},"outputs":[],"source":["\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM,Dense\n","from tensorflow.keras.callbacks import TensorBoard"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zHjGGmZXlxyN"},"outputs":[],"source":["log_dir = os.path.join('Logs')\n","tb_callback = TensorBoard(log_dir=log_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XhteWqrzlxyN"},"outputs":[],"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, BatchNormalization\n","#from tensorflow.keras.optimizers import Adam\n","\n","# Define the model\n","model = Sequential()\n","\n","# LSTM layers with a moderate number of units, using Batch Normalization\n","model.add(LSTM(64, return_sequences=True, input_shape=(30, 258)))  # 30 frames, 258 features\n","model.add(BatchNormalization())\n","model.add(LSTM(128,return_sequences=True))  # No return_sequences since we're only interested in final prediction\n","model.add(BatchNormalization())\n","model.add(LSTM(64))\n","model.add(BatchNormalization())\n","# Dense layers for classification\n","model.add(Dense(32, activation='relu'))\n","model.add(Dense(16, activation='relu'))\n","model.add(Dense(4, activation='softmax'))  # Output layer for 4 gesture categories\n","\n","# Compile the model\n","optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=1e-4, clipnorm=1.0)\n","model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LraGOmF-lxyP"},"outputs":[],"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, GRU, BatchNormalization, Bidirectional\n","\n","# Define the model\n","model = Sequential()\n","\n","# 2 DNN Layers\n","#model.add(Dense(64, activation='relu', input_shape=(45, 1662)))\n","model.add(Dense(128, activation='relu',input_shape=(45, 126)))\n","model.add(Dense(64, activation='relu'))\n","# 3 Bidirectional GRU Layers\n","model.add(Bidirectional(GRU(128, return_sequences=True)))\n","#model.add(BatchNormalization())  # Between GRU layers\n","\n","model.add(GRU(64, return_sequences=True))\n","#model.add(BatchNormalization())\n","\n","model.add(GRU(32))\n","#model.add(BatchNormalization())\n","\n","# Final Dense Layer for Classification\n","model.add(Dense(32,activation='relu'))\n","model.add(Dense(8, activation='softmax'))  # Change to 8 if you have 8 classes\n","\n","# Compile the model\n","model.compile(optimizer='adam',loss='categorical_crossentropy',  metrics=['accuracy'])\n","\n","# Model summary\n","model.summary()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jf0x3NIHlxyP","outputId":"7fb83260-808a-4353-e33c-f8f6c484d2d0"},"outputs":[{"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mlegacy\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m],)\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}],"source":["optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=1e-4)\n","\n","model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'],)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Gf6b50qlxyP"},"outputs":[],"source":["model.fit(X_train, y_train, epochs=50,batch_size=4,callbacks=[tb_callback])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wxDs-XtklxyP"},"outputs":[],"source":["del model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RMgPYN6GlxyQ"},"outputs":[],"source":["model=tf.keras.models.load_model(\"UmaKeshav.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zGBMAfiLlxyQ","outputId":"5687539b-d4f3-4612-adaa-72f47e08e930"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_2 (Dense)             (None, 45, 128)           16256     \n","                                                                 \n"," dense_3 (Dense)             (None, 45, 64)            8256      \n","                                                                 \n"," bidirectional (Bidirection  (None, 45, 256)           148992    \n"," al)                                                             \n","                                                                 \n"," gru_3 (GRU)                 (None, 45, 64)            61824     \n","                                                                 \n"," gru_4 (GRU)                 (None, 32)                9408      \n","                                                                 \n"," dense_4 (Dense)             (None, 32)                1056      \n","                                                                 \n"," dense_5 (Dense)             (None, 8)                 264       \n","                                                                 \n","=================================================================\n","Total params: 246056 (961.16 KB)\n","Trainable params: 246056 (961.16 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tYAFMgrHlxyQ","outputId":"bb79ced6-f9c2-4bc0-da19-a4fd0c432ef9"},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 1s 701ms/step\n"]}],"source":["res = model.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tmCcTH9olxyf","outputId":"a761adf8-3d01-4a97-be63-7c3eec844c00"},"outputs":[{"name":"stdout","output_type":"stream","text":["I Love You-->I Love You\n","How Are You-->How Are You\n","Hi-->Hi\n","Good Morning-->Good Morning\n","Sorry-->Sorry\n","I Love You-->I Love You\n","India-->India\n","India-->India\n","Hi-->I Love You\n","Sorry-->Sorry\n","Good Morning-->How Are You\n","How Are You-->How Are You\n","I Love You-->I Love You\n","India-->India\n","Sorry-->Sorry\n","Hi-->Hi\n"]}],"source":["\n","for i in range(16):\n","    print(actions[np.argmax(res[i])],end=\"-->\")\n","    print(actions[np.argmax(y_test[i])])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sIx3bqyelxyh","outputId":"cec95c02-f70b-4200-fed8-808928b76237"},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 54ms/step - loss: 0.4585 - accuracy: 0.8750\n"]}],"source":["eval_result = model.evaluate(X_test, y_test)\n","#print(f\"Test Loss: {eval_result[0]}, Test Accuracy: {eval_result[1]}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AOMwVHCllxyi"},"outputs":[],"source":["for i in range(16):\n","    print(actions[np.argmax(y_test[i])])\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AzQEKPN9lxyj"},"outputs":[],"source":["model.save(\"UmaKeshav.h5\")"]},{"cell_type":"markdown","metadata":{"id":"-rHhIKEwlxyj"},"source":["# Real Time\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9HXTkZmnlxyj"},"outputs":[],"source":["colors = [\n","    (245, 117, 16),  # Orange\n","    (117, 245, 16),  # Green\n","    (16, 117, 245),  # Blue\n","    (245, 16, 117),  # Pink\n","    (117, 16, 245),  # Purple\n","    (16, 245, 117),  # Teal\n","    (245, 245, 16),  # Yellow\n","    (117, 16, 245),  # Purple\n","]\n","def prob_viz_top(res, actions, input_frame, colors):\n","    output_frame = input_frame.copy()\n","    top_idx = np.argmax(res)  # Index of the highest probability\n","    top_prob = res[top_idx]  # Value of the highest probability\n","\n","    # Display the top prediction and its probability\n","    cv2.rectangle(output_frame, (0, 60), (int(top_prob * 300), 100), colors[top_idx], -1)\n","    cv2.putText(output_frame, f\"{actions[top_idx]}: {top_prob:.2f}\", (10, 90),\n","                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n","\n","    return output_frame\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pkxrau3Llxyk"},"outputs":[],"source":["colors = [\n","    (245, 117, 16),  # Orange\n","    (117, 245, 16),  # Green\n","    (16, 117, 245),  # Blue\n","    (245, 16, 117),  # Pink\n","    (117, 16, 245),  # Purple\n","    (16, 245, 117),  # Teal\n","    (245, 245, 16),  # Yellow\n","    (117, 16, 245),  # Purple\n","]\n","\n","def prob_viz(res, actions, input_frame, colors):\n","    output_frame = input_frame.copy()\n","    for num, prob in enumerate(res):\n","        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n","        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n","\n","    return output_frame"]},{"cell_type":"markdown","metadata":{"id":"9zYazxSqlxyp"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FW0Nz9XTlxyq"},"outputs":[],"source":["sequence = []\n","sentence = []\n","predictions = []\n","threshold = 0.5\n","\n","cam = cv2.VideoCapture(0)\n","# Set mediapipe model\n","with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n","    while cam.isOpened():\n","\n","        # Read feed\n","        ret, frame = cam.read()\n","\n","        # Make detections\n","        image, results = mediapipe_detector(frame, holistic)\n","        #print(results)\n","\n","        # Draw landmarks\n","        draw_landmarks(image, results)\n","\n","        # 2. Prediction logic\n","        keypoints = extract_keypoints(results)\n","        sequence.append(keypoints)\n","        sequence = sequence[-45:]\n","\n","        if len(sequence) == 45:\n","            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n","            #print(actions[np.argmax(res)])\n","            predictions.append(np.argmax(res))\n","\n","\n","        #3. Viz logic\n","            if np.unique(predictions[-10:])[0]==np.argmax(res):\n","                if res[np.argmax(res)] > threshold:\n","\n","                    if len(sentence) > 0:\n","                        if actions[np.argmax(res)] != sentence[-1]:\n","                            sentence.append(actions[np.argmax(res)])\n","                    else:\n","                        sentence.append(actions[np.argmax(res)])\n","\n","            if len(sentence) > 5:\n","                sentence = sentence[-5:]\n","\n","            # Viz probabilities\n","            image = prob_viz(res, actions, image, colors)\n","\n","        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n","        cv2.putText(image, ' '.join(sentence), (3,30),\n","                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n","\n","        # Show to screen\n","        cv2.imshow('OpenCV Feed', image)\n","\n","        # Break gracefully\n","        if cv2.waitKey(10) & 0xFF == ord('q'):\n","            break\n","    cam.release()\n","    cv2.destroyAllWindows()\n","    cv2.waitKey(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wHDLvO5clxyt"},"outputs":[],"source":["cap.release()\n","cv2.destroyAllWindows()\n","cv2.waitKey(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZxC95M4Nlxyu"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Njk6OuOglxyv","outputId":"90b48296-0b74-4128-b2ac-5571b7c55bf4"},"outputs":[{"ename":"AttributeError","evalue":"'str' object has no attribute 'call'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      3\u001b[0m converter \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mlite\u001b[38;5;241m.\u001b[39mTFLiteConverter\u001b[38;5;241m.\u001b[39mfrom_keras_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUmaKeshav.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m tflite_model \u001b[38;5;241m=\u001b[39m \u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgesture_model.tflite\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      7\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(tflite_model)\n","File \u001b[0;32m~/Programs/Git_Folder/Mini-project-Sem5/.venv/lib/python3.9/site-packages/tensorflow/lite/python/lite.py:1125\u001b[0m, in \u001b[0;36m_export_metrics.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(convert_func)\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1124\u001b[0m   \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m-> 1125\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_and_export_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Programs/Git_Folder/Mini-project-Sem5/.venv/lib/python3.9/site-packages/tensorflow/lite/python/lite.py:1079\u001b[0m, in \u001b[0;36mTFLiteConverterBase._convert_and_export_metrics\u001b[0;34m(self, convert_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1077\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_conversion_params_metric()\n\u001b[1;32m   1078\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mprocess_time()\n\u001b[0;32m-> 1079\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1080\u001b[0m elapsed_time_ms \u001b[38;5;241m=\u001b[39m (time\u001b[38;5;241m.\u001b[39mprocess_time() \u001b[38;5;241m-\u001b[39m start_time) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n","File \u001b[0;32m~/Programs/Git_Folder/Mini-project-Sem5/.venv/lib/python3.9/site-packages/tensorflow/lite/python/lite.py:1592\u001b[0m, in \u001b[0;36mTFLiteKerasModelConverterV2.convert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m saved_model_convert_result:\n\u001b[1;32m   1589\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m saved_model_convert_result\n\u001b[1;32m   1591\u001b[0m graph_def, input_tensors, output_tensors, frozen_func \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1592\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_freeze_keras_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1593\u001b[0m )\n\u001b[1;32m   1595\u001b[0m graph_def \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimize_tf_model(\n\u001b[1;32m   1596\u001b[0m     graph_def, input_tensors, output_tensors, frozen_func\n\u001b[1;32m   1597\u001b[0m )\n\u001b[1;32m   1599\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(TFLiteKerasModelConverterV2, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mconvert(\n\u001b[1;32m   1600\u001b[0m     graph_def, input_tensors, output_tensors\n\u001b[1;32m   1601\u001b[0m )\n","File \u001b[0;32m~/Programs/Git_Folder/Mini-project-Sem5/.venv/lib/python3.9/site-packages/tensorflow/lite/python/convert_phase.py:215\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m    214\u001b[0m   report_error_message(\u001b[38;5;28mstr\u001b[39m(error))\n\u001b[0;32m--> 215\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m error \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/Programs/Git_Folder/Mini-project-Sem5/.venv/lib/python3.9/site-packages/tensorflow/lite/python/convert_phase.py:205\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    204\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m ConverterError \u001b[38;5;28;01mas\u001b[39;00m converter_error:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m converter_error\u001b[38;5;241m.\u001b[39merrors:\n","File \u001b[0;32m~/Programs/Git_Folder/Mini-project-Sem5/.venv/lib/python3.9/site-packages/tensorflow/lite/python/lite.py:1529\u001b[0m, in \u001b[0;36mTFLiteKerasModelConverterV2._freeze_keras_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1523\u001b[0m input_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;66;03m# If the model's call is not a `tf.function`, then we need to first get its\u001b[39;00m\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;66;03m# input signature from `model_input_signature` method. We can't directly\u001b[39;00m\n\u001b[1;32m   1526\u001b[0m \u001b[38;5;66;03m# call `trace_model_call` because otherwise the batch dimension is set\u001b[39;00m\n\u001b[1;32m   1527\u001b[0m \u001b[38;5;66;03m# to None.\u001b[39;00m\n\u001b[1;32m   1528\u001b[0m \u001b[38;5;66;03m# Once we have better support for dynamic shapes, we can remove this.\u001b[39;00m\n\u001b[0;32m-> 1529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_keras_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m, _def_function\u001b[38;5;241m.\u001b[39mFunction):\n\u001b[1;32m   1530\u001b[0m   \u001b[38;5;66;03m# Pass `keep_original_batch_size=True` will ensure that we get an input\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m   \u001b[38;5;66;03m# signature including the batch dimension specified by the user.\u001b[39;00m\n\u001b[1;32m   1532\u001b[0m   \u001b[38;5;66;03m# TODO(b/169898786): Use the Keras public API when TFLite moves out of TF\u001b[39;00m\n\u001b[1;32m   1533\u001b[0m   input_signature \u001b[38;5;241m=\u001b[39m _model_input_signature(\n\u001b[1;32m   1534\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_keras_model, keep_original_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1535\u001b[0m   )\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# TODO(b/169898786): Use the Keras public API when TFLite moves out of TF\u001b[39;00m\n","\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'call'"]}],"source":["import tensorflow as tf\n","\n","converter = tf.lite.TFLiteConverter.from_keras_model('UmaKeshav.h5')\n","tflite_model = converter.convert()\n","\n","with open('gesture_model.tflite', 'wb') as f:\n","    f.write(tflite_model)\n"]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}